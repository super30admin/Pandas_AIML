{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pandas Lab — FAANG-Level Hands-On\n",
        "\n",
        "**Goal:** Build strong intuition for Pandas transformations used in ML pipelines (joins, groupby, leakage-safe features).\n",
        "\n",
        "**Outcome:** You can write correct, scalable Pandas code and explain common gotchas (join explosion, leakage, apply misuse).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0 — Synthetic Dataset (Interview-Friendly)\n",
        "We’ll use synthetic tables to mirror typical ML feature engineering tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(   user_id country  signup_ts\n",
              " 0      101      IN 2025-01-01\n",
              " 1      102      US 2025-01-03\n",
              " 2      103      IN 2025-01-04\n",
              " 3      104      CA 2025-01-10,\n",
              "    user_id   event_ts        event  amount\n",
              " 0      104 2025-01-17         view     0.0\n",
              " 1      103 2025-01-04         view     0.0\n",
              " 2      103 2025-01-02         view     0.0\n",
              " 3      102 2025-01-18  add_to_cart     0.0\n",
              " 4      102 2025-01-01         view     0.0)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rng = np.random.default_rng(0)\n",
        "\n",
        "users = pd.DataFrame({\n",
        "    'user_id': [101, 102, 103, 104],\n",
        "    'country': ['IN', 'US', 'IN', 'CA'],\n",
        "    'signup_ts': pd.to_datetime(['2025-01-01', '2025-01-03', '2025-01-04', '2025-01-10'])\n",
        "})\n",
        "\n",
        "events = pd.DataFrame({\n",
        "    'user_id': rng.choice(users['user_id'], size=30, replace=True),\n",
        "    'event_ts': pd.to_datetime('2025-01-01') + pd.to_timedelta(rng.integers(0, 20, size=30), unit='D'),\n",
        "    'event': rng.choice(['view', 'add_to_cart', 'purchase'], size=30, replace=True, p=[0.7, 0.2, 0.1]),\n",
        "    'amount': rng.choice([0, 0, 0, 19.99, 49.99, 99.99], size=30, replace=True),\n",
        "})\n",
        "\n",
        "# Ensure amount > 0 only for purchases (simple realism)\n",
        "events.loc[events['event'] != 'purchase', 'amount'] = 0\n",
        "\n",
        "users.head(), events.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 — DataFrame Fundamentals\n",
        "\n",
        "### Task 1.1: Basic filtering + new columns\n",
        "\n",
        "Create:\n",
        "- `purchases`: rows where `event == 'purchase'`\n",
        "- add column `event_day` = date (no time)\n",
        "\n",
        "# HINT:\n",
        "- Use boolean indexing\n",
        "- Use `.dt.floor('D')` or `.dt.date` (but prefer datetime64)\n",
        "\n",
        "**Explain:** Why does vectorized `.dt` beat per-row parsing?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# .dt is faster because it uses vectorized, C-level datetime operations instead of Python loops and function calls per row. Pandas stores datetimes as NumPy arrays, .dt accesses these arrays directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   user_id   event_ts     event  amount  event_day\n",
            "8      101 2025-01-10  purchase   99.99 2025-01-10\n",
            "OK: purchases_only\n",
            "OK: event_day_dtype\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "purchases = events[events['event'] == 'purchase']\n",
        "purchases = purchases.copy()\n",
        "# purchases\n",
        "purchases['event_day'] = purchases['event_ts'].dt.floor('D')\n",
        "\n",
        "print(purchases)\n",
        "\n",
        "check('purchases_only', set(purchases['event'].unique()) <= {'purchase'})\n",
        "check('event_day_dtype', str(purchases['event_day'].dtype).startswith('datetime'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 — GroupBy (Core for Features)\n",
        "\n",
        "### Task 2.1: Per-user aggregates\n",
        "Compute per-user features:\n",
        "- `n_events`\n",
        "- `n_purchases`\n",
        "- `total_revenue` (sum of amount)\n",
        "\n",
        "# HINT:\n",
        "- Use `groupby('user_id').agg(...)`\n",
        "- For `n_purchases`, use conditional aggregation\n",
        "\n",
        "**FAANG gotcha:** Counting purchases using `amount>0` vs `event=='purchase'` can diverge if data is messy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   user_id  n_events  n_purchases  total_revenue\n",
            "0      101         6            1          99.99\n",
            "1      102         4            0           0.00\n",
            "2      103        12            0           0.00\n",
            "3      104         8            0           0.00\n",
            "OK: has_user_id\n",
            "OK: has_total_revenue\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "user_features = (\n",
        "    events.assign(is_purchase = (events['event'] == 'purchase').astype(int))\n",
        "        .groupby('user_id', as_index=True)\n",
        "        .agg(\n",
        "            n_events = ('event', 'size'),\n",
        "            n_purchases = ('is_purchase', 'sum'),\n",
        "            total_revenue = ('amount', 'sum')\n",
        "        )\n",
        ")\n",
        "\n",
        "user_features = user_features.reset_index()\n",
        "print(user_features)\n",
        "check('has_user_id', 'user_id' in user_features.columns)\n",
        "check('has_total_revenue', 'total_revenue' in user_features.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.2: GroupBy transform (row-level feature)\n",
        "\n",
        "Add a column to `events` called `user_event_count` = number of events for that user.\n",
        "\n",
        "# HINT:\n",
        "- Use `groupby(...).transform('count')`\n",
        "\n",
        "**Explain:** Why is `transform` different from `agg`?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 'transform' and 'agg' are both used with groupby, but they serve different purposes and return different shapes. 'agg' reduces groups, and 'transform' preserves the original row count. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: user_event_count_nonnull\n",
            "OK: counts_positive\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "events2 = events.copy()\n",
        "events2['user_event_count'] = events.groupby('user_id')['event'].transform('size')\n",
        "#print(events2)\n",
        "\n",
        "check('user_event_count_nonnull', events2['user_event_count'].notna().all())\n",
        "check('counts_positive', (events2['user_event_count'] > 0).all())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 — Joins & Merge (Feature Table Building)\n",
        "\n",
        "### Task 3.1: Join user features back to users\n",
        "\n",
        "Create `user_table` by joining `users` with `user_features` on `user_id`.\n",
        "\n",
        "# HINT:\n",
        "- Use `merge(..., how='left')`\n",
        "- Fill missing aggregates with 0\n",
        "\n",
        "**FAANG gotcha:** If you accidentally do a many-to-many join, row count explodes. Always validate row counts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: rowcount_preserved\n",
            "   user_id country  signup_ts  n_events  n_purchases  total_revenue\n",
            "0      101      IN 2025-01-01         6            1          99.99\n",
            "1      102      US 2025-01-03         4            0           0.00\n",
            "2      103      IN 2025-01-04        12            0           0.00\n",
            "3      104      CA 2025-01-10         8            0           0.00\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "user_table = users.merge(user_features, on='user_id', how='left')\n",
        "\n",
        "# Fill NaNs for users with no events\n",
        "for col in ['n_events', 'n_purchases', 'total_revenue']:\n",
        "    if col in user_table.columns:\n",
        "        user_table[col] = user_table[col].fillna(0)\n",
        "\n",
        "check('rowcount_preserved', len(user_table) == len(users))\n",
        "print(user_table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3.2: Join explosion debugging (mini)\n",
        "\n",
        "Construct a tiny example where a join becomes many-to-many and explodes rows. Then fix it.\n",
        "\n",
        "# HINT:\n",
        "- Create `left` with duplicate keys\n",
        "- Create `right` with duplicate keys\n",
        "- Merge and observe rowcount\n",
        "\n",
        "**Explain:** What join cardinality do you expect (1:1, 1:n, n:1, n:n)?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Join cardinality is the expected relationship between rows on the left and right tables for the join keys, i.e., how many rows can match per key on each side. This is expected to avoid explosions and leakage.\n",
        "# 1:1 (one-to-one) - each key appears once in both tables.\n",
        "# 1:n (one-to-many) - key is unique on the left, may repeat on the right\n",
        "# n:1 (many-to-one) - key may repeat on the left, but is unique on the right\n",
        "# n:n (many-to-many) - keys repeat on both sides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "left rows 3 right rows 3 merged rows 5\n",
            "fixed rows 3\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "left = pd.DataFrame({'k': [1, 1, 2], 'l': ['a', 'b', 'c']})\n",
        "right = pd.DataFrame({'k': [1, 1, 2], 'r': ['x', 'y', 'z']})\n",
        "exploded = left.merge(right, on='k', how='inner')\n",
        "print('left rows', len(left), 'right rows', len(right), 'merged rows', len(exploded))\n",
        "\n",
        "# TODO: Fix (example: deduplicate right to 1 row per k)\n",
        "right_dedup = right.drop_duplicates('k', keep='first')\n",
        "fixed = left.merge(right_dedup, on='k', how='inner')\n",
        "print('fixed rows', len(fixed))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4 — Time-based Features & Leakage (FAANG System Thinking)\n",
        "\n",
        "### Task 4.1: Leakage-safe feature\n",
        "\n",
        "For each user and each event row, compute `purchases_before` = number of purchases strictly BEFORE that `event_ts`.\n",
        "\n",
        "Constraints:\n",
        "- No Python loops over rows\n",
        "- Must be time-aware\n",
        "\n",
        "# HINT:\n",
        "- Sort by user_id, event_ts\n",
        "- Create boolean `is_purchase`\n",
        "- Use groupby + cumsum, then shift\n",
        "\n",
        "**FAANG gotcha:** If you include the current row in the count, you leak label info for purchase events.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: nonnegative\n",
            "   user_id   event_ts        event  purchases_before\n",
            "0      101 2025-01-02         view                 0\n",
            "1      101 2025-01-06  add_to_cart                 0\n",
            "2      101 2025-01-10     purchase                 0\n",
            "3      101 2025-01-11         view                 1\n",
            "4      101 2025-01-14  add_to_cart                 1\n",
            "5      101 2025-01-20         view                 1\n",
            "6      102 2025-01-01         view                 0\n",
            "7      102 2025-01-16  add_to_cart                 0\n",
            "8      102 2025-01-17         view                 0\n",
            "9      102 2025-01-18  add_to_cart                 0\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "events3 = events.copy().sort_values(['user_id', 'event_ts']).reset_index(drop=True)\n",
        "events3['is_purchase'] = (events3['event'] == 'purchase').astype(int)\n",
        "\n",
        "# purchases up to and including current row\n",
        "events3['purchases_cum'] = events3.groupby('user_id')['is_purchase'].cumsum()\n",
        "\n",
        "# TODO: leakage-safe: purchases strictly before current row\n",
        "events3['purchases_before'] = events3.groupby('user_id')['purchases_cum'].shift(1).fillna(0).astype(int)\n",
        "\n",
        "check('nonnegative', (events3['purchases_before'] >= 0).all())\n",
        "print(events3[['user_id','event_ts','event','purchases_before']].head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5 — Apply vs Vectorization\n",
        "\n",
        "### Task 5.1: Remove slow apply\n",
        "\n",
        "Create a `country_is_in` boolean column on `users`.\n",
        "- First: do it with `.apply(...)` (slow style)\n",
        "- Then: replace it with vectorized code\n",
        "\n",
        "**Explain:** Why is `.apply` often slower in Pandas?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# .apply() is often slower in pandas because it executes Python code row-by-row, while vectorized operations run compiled C/NumPy code on whole arrays at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t_apply:  0.0011799335479736328\n",
            "t_vectorized:  0.0007810592651367188\n",
            "OK: same_result\n",
            "   user_id country  signup_ts  country_is_in_apply  country_is_in\n",
            "0      101      IN 2025-01-01                 True           True\n",
            "1      102      US 2025-01-03                False          False\n",
            "2      103      IN 2025-01-04                 True           True\n",
            "3      104      CA 2025-01-10                False          False\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "u = users.copy()\n",
        "\n",
        "# TODO: slow style (keep for comparison)\n",
        "t1 = time.time()\n",
        "u['country_is_in_apply'] = u['country'].apply(lambda x: x == 'IN')\n",
        "t_apply = time.time() - t1\n",
        "\n",
        "\n",
        "# TODO: vectorized style\n",
        "t2 = time.time()\n",
        "u['country_is_in'] = u['country'] == 'IN'\n",
        "t_vectorized = time.time() - t2\n",
        "\n",
        "print('t_apply: ', t_apply)\n",
        "print('t_vectorized: ', t_vectorized)\n",
        "check('same_result', (u['country_is_in_apply'] == u['country_is_in']).all())\n",
        "print(u)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- All checks pass\n",
        "- Explain prompts answered\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
